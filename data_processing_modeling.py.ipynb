{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc202959",
   "metadata": {},
   "source": [
    "Notes: This a notebook showing the data processing and modeling based on a **make-up synthetic dataset.**\n",
    "\n",
    "The purpose of this notebook is to show how the code will work to support our paper \"Does Location Matter? The Dominant Role of Temporal Features in Predicting Smoking Events\", ensuring the reproducibility of our work. The results do not present the real content of our actual results but the format and frame of it.\n",
    "\n",
    "If you have any questions, feel free to report an issue within our github repository, or contacting yang8597@umn.edu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6d0f43",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92cef782",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 74204.68it/s]\n"
     ]
    }
   ],
   "source": [
    "from functions import *\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "df_merged = pd.read_csv('./synthetic_raw_data.csv')\n",
    "\n",
    "quarter_hours = process_timestamps(df_merged, 'timestamp')\n",
    "df_quarter_hours = pd.DataFrame(quarter_hours, columns=['day_of_week', 'is_weekend', 'season'] + [f'time_quarter_{i}' for i in range(96)])\n",
    "\n",
    "df_merged_1 = pd.concat([df_merged, df_quarter_hours], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76a5275f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 46/46 [00:00<00:00, 138.23it/s]\n"
     ]
    }
   ],
   "source": [
    "df_merged_2 = process_location_info(df_merged_1, 'person_id', 'longitude', 'latitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fff87336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({nan: 40139, 'cigarette': 9861})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(df_merged_2['substance'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd01385",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "946d5af9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 106),\n",
       " Index(['person_id', 'timestamp', 'longitude', 'latitude', 'substance',\n",
       "        'is_after_covid', 'day_of_week', 'is_weekend', 'season',\n",
       "        'time_quarter_0',\n",
       "        ...\n",
       "        'time_quarter_87', 'time_quarter_88', 'time_quarter_89',\n",
       "        'time_quarter_90', 'time_quarter_91', 'time_quarter_92',\n",
       "        'time_quarter_93', 'time_quarter_94', 'time_quarter_95',\n",
       "        'cluster_label'],\n",
       "       dtype='object', length=106),\n",
       "   person_id   timestamp   longitude   latitude  substance  is_after_covid  \\\n",
       " 0   id0y9do  1643639679 -112.409096  34.456482          0               0   \n",
       " 1   id0y9do  1643639681 -112.179556  34.880768          0               0   \n",
       " 2   id0y9do  1643639697 -111.601791  34.599296          0               0   \n",
       " 3   id0y9do  1643639714 -112.349110  34.955633          0               0   \n",
       " 4   id0y9do  1643639773 -113.090380  34.687343          0               0   \n",
       " \n",
       "    day_of_week  is_weekend  season  time_quarter_0  ...  time_quarter_87  \\\n",
       " 0            0           0       0               0  ...                0   \n",
       " 1            0           0       0               0  ...                0   \n",
       " 2            0           0       0               0  ...                0   \n",
       " 3            0           0       0               0  ...                0   \n",
       " 4            0           0       0               0  ...                0   \n",
       " \n",
       "    time_quarter_88  time_quarter_89  time_quarter_90  time_quarter_91  \\\n",
       " 0                0                0                0                0   \n",
       " 1                0                0                0                0   \n",
       " 2                0                0                0                0   \n",
       " 3                0                0                0                0   \n",
       " 4                0                0                0                0   \n",
       " \n",
       "    time_quarter_92  time_quarter_93  time_quarter_94  time_quarter_95  \\\n",
       " 0                0                0                0                0   \n",
       " 1                0                0                0                0   \n",
       " 2                0                0                0                0   \n",
       " 3                0                0                0                0   \n",
       " 4                0                0                0                0   \n",
       " \n",
       "    cluster_label  \n",
       " 0           -1.0  \n",
       " 1           -1.0  \n",
       " 2           -1.0  \n",
       " 3           -1.0  \n",
       " 4           -1.0  \n",
       " \n",
       " [5 rows x 106 columns],\n",
       " Counter({'id0y9do': 1101,\n",
       "          'id1iblj': 1719,\n",
       "          'id1t2ta': 1547,\n",
       "          'id30t9n': 397,\n",
       "          'id3zmf8': 921,\n",
       "          'id5jsg6': 1816,\n",
       "          'id5kxvf': 1563,\n",
       "          'id5tb94': 914,\n",
       "          'id7ur23': 1513,\n",
       "          'id874fr': 578,\n",
       "          'id8ut0c': 539,\n",
       "          'id9uzfk': 1217,\n",
       "          'id9xpse': 111,\n",
       "          'id9xuy4': 1247,\n",
       "          'idazytj': 1941,\n",
       "          'idciyhe': 417,\n",
       "          'idcwi64': 324,\n",
       "          'iddw2pc': 319,\n",
       "          'idgdppq': 1469,\n",
       "          'idh75lx': 1983,\n",
       "          'idh9sdb': 1859,\n",
       "          'idhocn9': 1616,\n",
       "          'idhsahx': 291,\n",
       "          'idht0hl': 557,\n",
       "          'ididkwn': 1526,\n",
       "          'idimvih': 1594,\n",
       "          'idj2qp8': 1890,\n",
       "          'idla753': 799,\n",
       "          'idlc58d': 1568,\n",
       "          'idm5igq': 436,\n",
       "          'idmdd4v': 1714,\n",
       "          'idn9t84': 2000,\n",
       "          'idnhj7x': 455,\n",
       "          'ido6qji': 583,\n",
       "          'idpki7p': 127,\n",
       "          'idrc11e': 1926,\n",
       "          'idrtj5p': 1369,\n",
       "          'idt3w5u': 848,\n",
       "          'idthv3a': 1129,\n",
       "          'idujv6o': 1820,\n",
       "          'idvg0fn': 548,\n",
       "          'idvs4f8': 996,\n",
       "          'idxaji0': 747,\n",
       "          'idxepq8': 543,\n",
       "          'idy6dpb': 303,\n",
       "          'idzbikc': 1120}),\n",
       " 46)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, f1_score, accuracy_score, balanced_accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "df_modeling = df_merged_2.drop_duplicates()\n",
    "df_modeling['substance'] = df_modeling['substance'].apply(lambda x: 0 if pd.isna(x) else 1)\n",
    "df_modeling.shape, df_modeling.columns, df_modeling.head(), Counter(df_modeling['person_id']), len(set(df_modeling['person_id']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56de8915",
   "metadata": {},
   "source": [
    "### Correlation analysis between geo and temporal feature, mixed lm\n",
    "Examplified with smoking group, in quarter-hour intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b423c297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 46/46 [00:00<00:00, 797.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 99/99 [00:00<00:00, 1084.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model fitting failed for day_of_week: negative dimensions are not allowed\n",
      "Model fitting failed for is_weekend: negative dimensions are not allowed\n",
      "Model fitting failed for season: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_0: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_1: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_2: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_3: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_4: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_5: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_6: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_7: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_8: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_9: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_10: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_11: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_12: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_13: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_14: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_15: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_16: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_17: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_18: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_19: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_20: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_21: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_22: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_23: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_24: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_25: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_26: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_27: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_28: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_29: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_30: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_31: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_32: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_33: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_34: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_35: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_36: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_37: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_38: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_39: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_40: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_41: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_42: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_43: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_44: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_45: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_46: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_47: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_48: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_49: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_50: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_51: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_52: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_53: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_54: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_55: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_56: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_57: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_58: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_59: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_60: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_61: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_62: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_63: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_64: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_65: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_66: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_67: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_68: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_69: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_70: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_71: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_72: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_73: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_74: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_75: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_76: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_77: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_78: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_79: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_80: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_81: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_82: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_83: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_84: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_85: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_86: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_87: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_88: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_89: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_90: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_91: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_92: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_93: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_94: negative dimensions are not allowed\n",
      "Model fitting failed for time_quarter_95: negative dimensions are not allowed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variable</th>\n",
       "      <th>Support</th>\n",
       "      <th>Coef.</th>\n",
       "      <th>Std.Err.</th>\n",
       "      <th>z</th>\n",
       "      <th>P&gt;|z|</th>\n",
       "      <th>0.025</th>\n",
       "      <th>0.975</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>day_of_week</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is_weekend</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>season</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>time_quarter_0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>time_quarter_1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>time_quarter_2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>time_quarter_3</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>time_quarter_4</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>time_quarter_5</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>time_quarter_6</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>time_quarter_7</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>time_quarter_8</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>time_quarter_9</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>time_quarter_10</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>time_quarter_11</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>time_quarter_12</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>time_quarter_13</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>time_quarter_14</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>time_quarter_15</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>time_quarter_16</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Variable  Support  Coef.  Std.Err.   z  P>|z|  0.025  0.975\n",
       "0       day_of_week        0    NaN       NaN NaN    NaN    NaN    NaN\n",
       "1        is_weekend        0    NaN       NaN NaN    NaN    NaN    NaN\n",
       "2            season        0    NaN       NaN NaN    NaN    NaN    NaN\n",
       "3    time_quarter_0        0    NaN       NaN NaN    NaN    NaN    NaN\n",
       "4    time_quarter_1        0    NaN       NaN NaN    NaN    NaN    NaN\n",
       "5    time_quarter_2        0    NaN       NaN NaN    NaN    NaN    NaN\n",
       "6    time_quarter_3        0    NaN       NaN NaN    NaN    NaN    NaN\n",
       "7    time_quarter_4        0    NaN       NaN NaN    NaN    NaN    NaN\n",
       "8    time_quarter_5        0    NaN       NaN NaN    NaN    NaN    NaN\n",
       "9    time_quarter_6        0    NaN       NaN NaN    NaN    NaN    NaN\n",
       "10   time_quarter_7        0    NaN       NaN NaN    NaN    NaN    NaN\n",
       "11   time_quarter_8        0    NaN       NaN NaN    NaN    NaN    NaN\n",
       "12   time_quarter_9        0    NaN       NaN NaN    NaN    NaN    NaN\n",
       "13  time_quarter_10        0    NaN       NaN NaN    NaN    NaN    NaN\n",
       "14  time_quarter_11        0    NaN       NaN NaN    NaN    NaN    NaN\n",
       "15  time_quarter_12        0    NaN       NaN NaN    NaN    NaN    NaN\n",
       "16  time_quarter_13        0    NaN       NaN NaN    NaN    NaN    NaN\n",
       "17  time_quarter_14        0    NaN       NaN NaN    NaN    NaN    NaN\n",
       "18  time_quarter_15        0    NaN       NaN NaN    NaN    NaN    NaN\n",
       "19  time_quarter_16        0    NaN       NaN NaN    NaN    NaN    NaN"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import mixedlm\n",
    "\n",
    "df_smk = df_modeling[df_modeling['substance']==1]\n",
    "df_smk.reset_index(inplace=True)\n",
    "del df_smk['index']\n",
    "\n",
    "df_smk0 = df_smk[['person_id','cluster_label', 'day_of_week', 'is_weekend', 'season', 'longitude', 'latitude', 'time_quarter_0', 'time_quarter_1', 'time_quarter_2', 'time_quarter_3', 'time_quarter_4', 'time_quarter_5', 'time_quarter_6', 'time_quarter_7', 'time_quarter_8', 'time_quarter_9', 'time_quarter_10', 'time_quarter_11', 'time_quarter_12', 'time_quarter_13', 'time_quarter_14', 'time_quarter_15', 'time_quarter_16', 'time_quarter_17', 'time_quarter_18', 'time_quarter_19', 'time_quarter_20', 'time_quarter_21', 'time_quarter_22', 'time_quarter_23', 'time_quarter_24', 'time_quarter_25', 'time_quarter_26', 'time_quarter_27', 'time_quarter_28', 'time_quarter_29', 'time_quarter_30', 'time_quarter_31', 'time_quarter_32', 'time_quarter_33', 'time_quarter_34', 'time_quarter_35', 'time_quarter_36', 'time_quarter_37', 'time_quarter_38', 'time_quarter_39', 'time_quarter_40', 'time_quarter_41', 'time_quarter_42', 'time_quarter_43', 'time_quarter_44', 'time_quarter_45', 'time_quarter_46', 'time_quarter_47', 'time_quarter_48', 'time_quarter_49', 'time_quarter_50', 'time_quarter_51', 'time_quarter_52', 'time_quarter_53', 'time_quarter_54', 'time_quarter_55', 'time_quarter_56', 'time_quarter_57', 'time_quarter_58', 'time_quarter_59', 'time_quarter_60', 'time_quarter_61', 'time_quarter_62', 'time_quarter_63', 'time_quarter_64', 'time_quarter_65', 'time_quarter_66', 'time_quarter_67', 'time_quarter_68', 'time_quarter_69', 'time_quarter_70', 'time_quarter_71', 'time_quarter_72', 'time_quarter_73', 'time_quarter_74', 'time_quarter_75', 'time_quarter_76', 'time_quarter_77', 'time_quarter_78', 'time_quarter_79', 'time_quarter_80', 'time_quarter_81', 'time_quarter_82', 'time_quarter_83', 'time_quarter_84', 'time_quarter_85', 'time_quarter_86', 'time_quarter_87', 'time_quarter_88', 'time_quarter_89', 'time_quarter_90', 'time_quarter_91', 'time_quarter_92', 'time_quarter_93', 'time_quarter_94', 'time_quarter_95']]\n",
    "print(len(set(df_smk0['person_id'])))\n",
    "# Get rid of the participants with only one cluster label\n",
    "pids = list(set(df_smk0['person_id']))\n",
    "df_smk1 = pd.DataFrame([], columns = df_smk0.columns)\n",
    "for idx,pid in enumerate(tqdm(pids)):\n",
    "    df_temp=df_smk0[df_smk0['person_id']==pid]\n",
    "    set_len_cluster = len(set(df_temp['cluster_label']))\n",
    "    if set_len_cluster<=1:\n",
    "        continue\n",
    "    df_smk1 = pd.concat([df_smk1,df_temp],axis=0)\n",
    "print(len(set(df_smk1['person_id'])))\n",
    "\n",
    "time_quarter_columns = [f\"time_quarter_{i}\" for i in range(96)]\n",
    "\n",
    "results_data=[]\n",
    "for col in tqdm(['day_of_week', 'is_weekend', 'season']+time_quarter_columns):\n",
    "    formula = f\"cluster_label ~ {col}\"\n",
    "    supp = len(df_smk1[df_smk1[col]!=0])\n",
    "    try:\n",
    "        # Fit the mixed-effects model\n",
    "        model = mixedlm(formula, df_smk1, groups=df_smk1[\"person_id\"])\n",
    "        result = model.fit()\n",
    "        # Extract coefficients and confidence intervals for each model\n",
    "        coef_summary = dict(result.summary().tables[1].iloc[1,:])  # Skip header row\n",
    "        coef_data = {\n",
    "            'Variable': col,\n",
    "            'Support': supp, \n",
    "            'Coef.': float(coef_summary['Coef.']),\n",
    "            'Std.Err.': float(coef_summary['Std.Err.']),\n",
    "            'z': float(coef_summary['z']),\n",
    "            'P>|z|': float(coef_summary['P>|z|']),\n",
    "            '0.025': float(coef_summary['[0.025']),\n",
    "            '0.975': float(coef_summary['0.975]'])\n",
    "        }\n",
    "        results_data.append(coef_data)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Model fitting failed for {col}: {e}\")\n",
    "        coef_data = {\n",
    "            'Variable': col,\n",
    "            'Support': supp, \n",
    "            'Coef.': np.nan,\n",
    "            'Std.Err.': np.nan,\n",
    "            'z': np.nan,\n",
    "            'P>|z|': np.nan,\n",
    "            '0.025': np.nan,\n",
    "            '0.975': np.nan\n",
    "        }\n",
    "        results_data.append(coef_data)\n",
    "\n",
    "# Convert the results into a DataFrame\n",
    "results_df = pd.DataFrame(results_data)\n",
    "\n",
    "results_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3e006f",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "Examplified with 15 minutes HALF_DURATION, i.e. one smoking event will last 30 minutes, centered the datetime point of the smoker's report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "70f646cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_all = ['day_of_week', 'is_weekend', 'season', 'time_quarter_0', 'time_quarter_1', 'time_quarter_2', 'time_quarter_3', 'time_quarter_4', 'time_quarter_5', 'time_quarter_6', 'time_quarter_7', 'time_quarter_8', 'time_quarter_9', 'time_quarter_10', 'time_quarter_11', 'time_quarter_12', 'time_quarter_13', 'time_quarter_14', 'time_quarter_15', 'time_quarter_16', 'time_quarter_17', 'time_quarter_18', 'time_quarter_19', 'time_quarter_20', 'time_quarter_21', 'time_quarter_22', 'time_quarter_23', 'time_quarter_24', 'time_quarter_25', 'time_quarter_26', 'time_quarter_27', 'time_quarter_28', 'time_quarter_29', 'time_quarter_30', 'time_quarter_31', 'time_quarter_32', 'time_quarter_33', 'time_quarter_34', 'time_quarter_35', 'time_quarter_36', 'time_quarter_37', 'time_quarter_38', 'time_quarter_39', 'time_quarter_40', 'time_quarter_41', 'time_quarter_42', 'time_quarter_43', 'time_quarter_44', 'time_quarter_45', 'time_quarter_46', 'time_quarter_47', 'time_quarter_48', 'time_quarter_49', 'time_quarter_50', 'time_quarter_51', 'time_quarter_52', 'time_quarter_53', 'time_quarter_54', 'time_quarter_55', 'time_quarter_56', 'time_quarter_57', 'time_quarter_58', 'time_quarter_59', 'time_quarter_60', 'time_quarter_61', 'time_quarter_62', 'time_quarter_63', 'time_quarter_64', 'time_quarter_65', 'time_quarter_66', 'time_quarter_67', 'time_quarter_68', 'time_quarter_69', 'time_quarter_70', 'time_quarter_71', 'time_quarter_72', 'time_quarter_73', 'time_quarter_74', 'time_quarter_75', 'time_quarter_76', 'time_quarter_77', 'time_quarter_78', 'time_quarter_79', 'time_quarter_80', 'time_quarter_81', 'time_quarter_82', 'time_quarter_83', 'time_quarter_84', 'time_quarter_85', 'time_quarter_86', 'time_quarter_87', 'time_quarter_88', 'time_quarter_89', 'time_quarter_90', 'time_quarter_91', 'time_quarter_92', 'time_quarter_93', 'time_quarter_94', 'time_quarter_95'\n",
    "                , 'cluster_label']\n",
    "features_wo_location = ['day_of_week', 'is_weekend', 'season', 'time_quarter_0', 'time_quarter_1', 'time_quarter_2', 'time_quarter_3', 'time_quarter_4', 'time_quarter_5', 'time_quarter_6', 'time_quarter_7', 'time_quarter_8', 'time_quarter_9', 'time_quarter_10', 'time_quarter_11', 'time_quarter_12', 'time_quarter_13', 'time_quarter_14', 'time_quarter_15', 'time_quarter_16', 'time_quarter_17', 'time_quarter_18', 'time_quarter_19', 'time_quarter_20', 'time_quarter_21', 'time_quarter_22', 'time_quarter_23', 'time_quarter_24', 'time_quarter_25', 'time_quarter_26', 'time_quarter_27', 'time_quarter_28', 'time_quarter_29', 'time_quarter_30', 'time_quarter_31', 'time_quarter_32', 'time_quarter_33', 'time_quarter_34', 'time_quarter_35', 'time_quarter_36', 'time_quarter_37', 'time_quarter_38', 'time_quarter_39', 'time_quarter_40', 'time_quarter_41', 'time_quarter_42', 'time_quarter_43', 'time_quarter_44', 'time_quarter_45', 'time_quarter_46', 'time_quarter_47', 'time_quarter_48', 'time_quarter_49', 'time_quarter_50', 'time_quarter_51', 'time_quarter_52', 'time_quarter_53', 'time_quarter_54', 'time_quarter_55', 'time_quarter_56', 'time_quarter_57', 'time_quarter_58', 'time_quarter_59', 'time_quarter_60', 'time_quarter_61', 'time_quarter_62', 'time_quarter_63', 'time_quarter_64', 'time_quarter_65', 'time_quarter_66', 'time_quarter_67', 'time_quarter_68', 'time_quarter_69', 'time_quarter_70', 'time_quarter_71', 'time_quarter_72', 'time_quarter_73', 'time_quarter_74', 'time_quarter_75', 'time_quarter_76', 'time_quarter_77', 'time_quarter_78', 'time_quarter_79', 'time_quarter_80', 'time_quarter_81', 'time_quarter_82', 'time_quarter_83', 'time_quarter_84', 'time_quarter_85', 'time_quarter_86', 'time_quarter_87', 'time_quarter_88', 'time_quarter_89', 'time_quarter_90', 'time_quarter_91', 'time_quarter_92', 'time_quarter_93', 'time_quarter_94', 'time_quarter_95']\n",
    "features_wo_time = ['day_of_week', 'is_weekend', 'season', 'cluster_label']\n",
    "after_covid_ids = list(set(df_modeling[df_modeling['is_after_covid']==1]))\n",
    "def process_feature_importance_structure(raw_feature_importance, person_id):\n",
    "    feature_importance_t = raw_feature_importance.set_index('feature').T.reset_index().rename(columns={'index':'person_id'})\n",
    "    feature_importance_t['person_id'] = person_id\n",
    "    feature_importance_tt = feature_importance_t.reset_index(drop=True)\n",
    "    feature_importance_tt.columns.names=[None]\n",
    "    return feature_importance_tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "001c3309",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 46/46 [00:46<00:00,  1.01s/it]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, f1_score, accuracy_score, balanced_accuracy_score\n",
    "from sklearn.model_selection import cross_val_predict, StratifiedKFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "person_ids = list(set(df_modeling['person_id']))\n",
    "\n",
    "def custom_scoring(y_true, y_pred, y_pred_prob):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    balanced_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "    ras = roc_auc_score(y_true, y_pred_prob)\n",
    "    return cm.flatten().tolist() + [macro_f1, ras, acc, balanced_acc]\n",
    "\n",
    "feature_importances_all = pd.DataFrame([], columns=['person_id'] + features_all)\n",
    "feature_importances_wo_location = pd.DataFrame([], columns=['person_id'] + features_wo_location)\n",
    "feature_importances_wo_time = pd.DataFrame([], columns=['person_id'] + features_wo_time)\n",
    "\n",
    "eva_columns = ['person_id', 'Is_after_covid','reported_case_counts', 'sample_size', 'positive_cases',\n",
    "               'all_features_confusion_matrix_00', 'all_features_confusion_matrix_01',\n",
    "                'all_features_confusion_matrix_10', 'all_features_confusion_matrix_11',\n",
    "                'all_features_macro_f1', 'all_features_roc_auc_score', 'all_features_accuracy', 'all_features_balanced_accuracy',\n",
    "                'wo_location_confusion_matrix_00', 'wo_location_confusion_matrix_01',\n",
    "                'wo_location_confusion_matrix_10', 'wo_location_confusion_matrix_11',\n",
    "                'wo_location_macro_f1', 'wo_location_roc_auc_score', 'wo_location_accuracy', 'wo_location_balanced_accuracy',\n",
    "                'wo_time_confusion_matrix_00', 'wo_time_confusion_matrix_01',\n",
    "                'wo_time_confusion_matrix_10', 'wo_time_confusion_matrix_11',\n",
    "                'wo_time_macro_f1', 'wo_time_roc_auc_score', 'wo_time_accuracy', 'wo_location_balanced_accuracy']\n",
    "evaluations = pd.DataFrame([], columns= eva_columns)\n",
    "for idx,p in enumerate(tqdm(person_ids)):\n",
    "    ll = len(df_merged_2[df_merged_2['person_id']==p])\n",
    "    # '3n49pu7','3wqwm47', # Too few samples to skip (*2), when 10min\n",
    "    # 'i4109ac' when both 10min and 15min, after last cigareet extraction\n",
    "    if p in ['n4h80yi', 'i5v361b',  '4u8ihdn', '006', 'tmz8lam', 'i4109ac']: # Too few samples to skip (*2); Only have one class in outcome (*3);\n",
    "        continue\n",
    "    # 1. All features\n",
    "    X = df_modeling[df_modeling['person_id']==p].drop(columns=['person_id', 'substance'])\n",
    "    y = df_modeling[df_modeling['person_id']==p]['substance']\n",
    "    if p in after_covid_ids:\n",
    "        iac = 1\n",
    "    else:\n",
    "        iac = 0\n",
    "    evaluations_list = [p, iac, ll, len(y), dict(Counter(y))[1]]\n",
    "#     # Split the data into train and test sets\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=23, stratify=y)\n",
    "#     # Handle imbalance using SMOTE (Synthetic Minority Over-sampling Technique)\n",
    "#     smote = SMOTE(random_state=42)\n",
    "#     X_train_sm, y_train_sm = smote.fit_resample(X_train, y_train)\n",
    "#     # Train the logistic regression model\n",
    "#     model = LogisticRegression(max_iter=3000, class_weight='balanced')\n",
    "#     model.fit(X_train_sm, y_train_sm)\n",
    "#     # Make predictions\n",
    "    # Create a stratified k-fold object. k=5 here\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # Define a pipeline with SMOTE and Logistic Regression\n",
    "    pipeline = Pipeline([\n",
    "        ('smote', SMOTE(random_state=42)),\n",
    "        ('logreg', LogisticRegression(max_iter=3000, class_weight='balanced'))\n",
    "    ])\n",
    "    \n",
    "    y_pred = cross_val_predict(pipeline, X, y, cv=cv, method='predict')\n",
    "    y_pred_prob = cross_val_predict(pipeline, X, y, cv=cv, method='predict_proba')[:, 1]\n",
    "\n",
    "#     # Evaluate the model\n",
    "#     cm = confusion_matrix(y_test, y_pred)\n",
    "#     evaluations_list+=list(cm.flatten())\n",
    "#     cr = classification_report(y_test, y_pred)\n",
    "#     macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "#     acc, balanced_acc = accuracy_score(y_test, y_pred), balanced_accuracy_score(y_test, y_pred)\n",
    "#     ras = roc_auc_score(y_test, y_pred_prob)\n",
    "#     evaluations_list+=[macro_f1, ras, acc, balanced_acc]\n",
    "#     # Feature importances\n",
    "#     feature_importance = pd.DataFrame({'feature': X.columns, 'importance': model.coef_[0]})\n",
    "#     tmp_feature_importances_all = process_feature_importance_structure(feature_importance, p)\n",
    "#     feature_importances_all = pd.concat([feature_importances_all, tmp_feature_importances_all], axis=0)\n",
    "    \n",
    "    # Evaluate the model using custom scoring\n",
    "    evaluation_metrics = custom_scoring(y, y_pred, y_pred_prob)\n",
    "    evaluations_list += evaluation_metrics\n",
    "\n",
    "    # Feature importances using the pipeline\n",
    "    pipeline.fit(X, y)  # Fit the pipeline once to get feature importances\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': pipeline.named_steps['logreg'].coef_[0]\n",
    "    })\n",
    "\n",
    "    # Process feature importances\n",
    "    tmp_feature_importances_all = process_feature_importance_structure(feature_importance, p)\n",
    "    feature_importances_all = pd.concat([feature_importances_all, tmp_feature_importances_all], axis=0)\n",
    "    \n",
    "    # 2. Without location information\n",
    "    X = df_modeling[df_modeling['person_id']==p].drop(columns=['person_id', 'substance', \n",
    "#                                                                'longitude', 'latitude',\n",
    "                                                               'cluster_label'])\n",
    "    y = df_modeling[df_modeling['person_id']==p]['substance']\n",
    "#     # Split the data into train and test sets\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=23, stratify=y)\n",
    "#     # Handle imbalance using SMOTE (Synthetic Minority Over-sampling Technique)\n",
    "#     smote = SMOTE(random_state=42)\n",
    "#     X_train_sm, y_train_sm = smote.fit_resample(X_train, y_train)\n",
    "#     # Train the logistic regression model\n",
    "#     model = LogisticRegression(max_iter=3000, class_weight='balanced')\n",
    "#     model.fit(X_train_sm, y_train_sm)\n",
    "#     # Make predictions\n",
    "#     y_pred = model.predict(X_test)\n",
    "#     y_pred_prob = model.predict_proba(X_test)[:, 1]\n",
    "#     # Evaluate the model\n",
    "#     cm = confusion_matrix(y_test, y_pred)\n",
    "#     evaluations_list+=list(cm.flatten())\n",
    "#     cr = classification_report(y_test, y_pred)\n",
    "#     macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "#     acc, balanced_acc = accuracy_score(y_test, y_pred), balanced_accuracy_score(y_test, y_pred)\n",
    "#     ras = roc_auc_score(y_test, y_pred_prob)\n",
    "#     evaluations_list+=[macro_f1, ras, acc, balanced_acc]\n",
    "#     # Feature importances\n",
    "#     feature_importance = pd.DataFrame({'feature': X.columns, 'importance': model.coef_[0]})\n",
    "#     tmp_feature_importances_wo_location = process_feature_importance_structure(feature_importance, p)\n",
    "#     feature_importances_wo_location = pd.concat([feature_importances_wo_location, tmp_feature_importances_wo_location], axis=0)\n",
    "    \n",
    "    # Create a stratified k-fold object. k=5 here\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # Define a pipeline with SMOTE and Logistic Regression\n",
    "    pipeline = Pipeline([\n",
    "        ('smote', SMOTE(random_state=42)),\n",
    "        ('logreg', LogisticRegression(max_iter=3000, class_weight='balanced'))\n",
    "    ])\n",
    "    \n",
    "    y_pred = cross_val_predict(pipeline, X, y, cv=cv, method='predict')\n",
    "    y_pred_prob = cross_val_predict(pipeline, X, y, cv=cv, method='predict_proba')[:, 1]\n",
    "    # Evaluate the model using custom scoring\n",
    "    evaluation_metrics = custom_scoring(y, y_pred, y_pred_prob)\n",
    "    evaluations_list += evaluation_metrics\n",
    "\n",
    "    # Feature importances using the pipeline\n",
    "    pipeline.fit(X, y)  # Fit the pipeline once to get feature importances\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': pipeline.named_steps['logreg'].coef_[0]\n",
    "    })\n",
    "\n",
    "    # Process feature importances\n",
    "    tmp_feature_importances_wo_location = process_feature_importance_structure(feature_importance, p)\n",
    "    feature_importances_wo_location = pd.concat([feature_importances_wo_location, tmp_feature_importances_wo_location], axis=0)\n",
    "    \n",
    "    \n",
    "    # 3. Without time information\n",
    "    X = df_modeling[df_modeling['person_id']==p].drop(columns=['person_id', 'substance', \n",
    "                                                               'time_quarter_0', 'time_quarter_1', 'time_quarter_2', 'time_quarter_3', \n",
    "                                                               'time_quarter_4', 'time_quarter_5', 'time_quarter_6', 'time_quarter_7', \n",
    "                                                               'time_quarter_8', 'time_quarter_9', 'time_quarter_10', 'time_quarter_11', \n",
    "                                                               'time_quarter_12', 'time_quarter_13', 'time_quarter_14', 'time_quarter_15', \n",
    "                                                               'time_quarter_16', 'time_quarter_17', 'time_quarter_18', 'time_quarter_19', \n",
    "                                                               'time_quarter_20', 'time_quarter_21', 'time_quarter_22', 'time_quarter_23', \n",
    "                                                               'time_quarter_24', 'time_quarter_25', 'time_quarter_26', 'time_quarter_27', \n",
    "                                                               'time_quarter_28', 'time_quarter_29', 'time_quarter_30', 'time_quarter_31', \n",
    "                                                               'time_quarter_32', 'time_quarter_33', 'time_quarter_34', 'time_quarter_35', \n",
    "                                                               'time_quarter_36', 'time_quarter_37', 'time_quarter_38', 'time_quarter_39', \n",
    "                                                               'time_quarter_40', 'time_quarter_41', 'time_quarter_42', 'time_quarter_43', \n",
    "                                                               'time_quarter_44', 'time_quarter_45', 'time_quarter_46', 'time_quarter_47', \n",
    "                                                               'time_quarter_48', 'time_quarter_49', 'time_quarter_50', 'time_quarter_51', \n",
    "                                                               'time_quarter_52', 'time_quarter_53', 'time_quarter_54', 'time_quarter_55', \n",
    "                                                               'time_quarter_56', 'time_quarter_57', 'time_quarter_58', 'time_quarter_59', \n",
    "                                                               'time_quarter_60', 'time_quarter_61', 'time_quarter_62', 'time_quarter_63', \n",
    "                                                               'time_quarter_64', 'time_quarter_65', 'time_quarter_66', 'time_quarter_67', \n",
    "                                                               'time_quarter_68', 'time_quarter_69', 'time_quarter_70', 'time_quarter_71', \n",
    "                                                               'time_quarter_72', 'time_quarter_73', 'time_quarter_74', 'time_quarter_75', \n",
    "                                                               'time_quarter_76', 'time_quarter_77', 'time_quarter_78', 'time_quarter_79', \n",
    "                                                               'time_quarter_80', 'time_quarter_81', 'time_quarter_82', 'time_quarter_83', \n",
    "                                                               'time_quarter_84', 'time_quarter_85', 'time_quarter_86', 'time_quarter_87', \n",
    "                                                               'time_quarter_88', 'time_quarter_89', 'time_quarter_90', 'time_quarter_91', \n",
    "                                                               'time_quarter_92', 'time_quarter_93', 'time_quarter_94', 'time_quarter_95'])\n",
    "\n",
    "    y = df_modeling[df_modeling['person_id']==p]['substance']\n",
    "#     # Split the data into train and test sets\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=23, stratify=y)\n",
    "#     # Handle imbalance using SMOTE (Synthetic Minority Over-sampling Technique)\n",
    "#     smote = SMOTE(random_state=42)\n",
    "#     X_train_sm, y_train_sm = smote.fit_resample(X_train, y_train)\n",
    "#     # Train the logistic regression model\n",
    "#     model = LogisticRegression(max_iter=3000, class_weight='balanced')\n",
    "#     model.fit(X_train_sm, y_train_sm)\n",
    "#     # Make predictions\n",
    "#     y_pred = model.predict(X_test)\n",
    "#     y_pred_prob = model.predict_proba(X_test)[:, 1]\n",
    "#     # Evaluate the model\n",
    "#     cm = confusion_matrix(y_test, y_pred)\n",
    "#     evaluations_list+=list(cm.flatten())\n",
    "#     cr = classification_report(y_test, y_pred)\n",
    "#     macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "#     acc, balanced_acc = accuracy_score(y_test, y_pred), balanced_accuracy_score(y_test, y_pred)\n",
    "#     ras = roc_auc_score(y_test, y_pred_prob)\n",
    "#     evaluations_list+=[macro_f1, ras, acc, balanced_acc]\n",
    "#     # Feature importances\n",
    "#     feature_importance = pd.DataFrame({'feature': X.columns, 'importance': model.coef_[0]})\n",
    "#     tmp_feature_importances_wo_time = process_feature_importance_structure(feature_importance, p)\n",
    "#     feature_importances_wo_time = pd.concat([feature_importances_wo_time, tmp_feature_importances_wo_time], axis=0)\n",
    "    \n",
    "    # Create a stratified k-fold object. k=5 here\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # Define a pipeline with SMOTE and Logistic Regression\n",
    "    pipeline = Pipeline([\n",
    "        ('smote', SMOTE(random_state=42)),\n",
    "        ('logreg', LogisticRegression(max_iter=3000, class_weight='balanced'))\n",
    "    ])\n",
    "    \n",
    "    y_pred = cross_val_predict(pipeline, X, y, cv=cv, method='predict')\n",
    "    y_pred_prob = cross_val_predict(pipeline, X, y, cv=cv, method='predict_proba')[:, 1]\n",
    "    # Evaluate the model using custom scoring\n",
    "    evaluation_metrics = custom_scoring(y, y_pred, y_pred_prob)\n",
    "    evaluations_list += evaluation_metrics\n",
    "\n",
    "    # Feature importances using the pipeline\n",
    "    pipeline.fit(X, y)  # Fit the pipeline once to get feature importances\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': pipeline.named_steps['logreg'].coef_[0]\n",
    "    })\n",
    "\n",
    "    # Process feature importances\n",
    "    tmp_feature_importances_wo_time = process_feature_importance_structure(feature_importance, p)\n",
    "    feature_importances_wo_time = pd.concat([feature_importances_wo_time, tmp_feature_importances_wo_time], axis=0)\n",
    "    \n",
    "    df_tmp_eva = pd.DataFrame([evaluations_list], columns=eva_columns)\n",
    "    evaluations = pd.concat([evaluations, df_tmp_eva], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dc5d241a",
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix = \"_15_min\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c2b47d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances_all.to_excel(f'./synthetic_data_5-fold_CV/synthetic_data_feature_importances_all_logistic_regression{suffix}.xlsx', index=False)   \n",
    "feature_importances_wo_location.to_excel(f'./synthetic_data_5-fold_CV/synthetic_data_feature_importances_wo_location_logistic_regression{suffix}.xlsx', index=False)\n",
    "feature_importances_wo_time.to_excel(f'./synthetic_data_5-fold_CV/synthetic_data_feature_importances_wo_time_logistic_regression{suffix}.xlsx', index=False) \n",
    "evaluations.to_excel(f'./synthetic_data_5-fold_CV/synthetic_data_Evaluation_metrics_logistic_regression{suffix}.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52213b7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
